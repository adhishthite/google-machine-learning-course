{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Regularization.ipynb",
      "version": "0.3.2",
      "views": {},
      "default_view": {},
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "metadata": {
        "id": "bOg53ry8mpda",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Deep Neural Network - L2 Regularization, Dropout, Learning Rate Decay\n",
        "\n",
        "---\n",
        "\n",
        "Created as a part of Google's 'Machine Learning to Deep Learning' course on Udacity.\n",
        "\n",
        "(Created with Google Colab)"
      ]
    },
    {
      "metadata": {
        "id": "l_QpjhyunAb3",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Step 1 - Imports and Data Setup"
      ]
    },
    {
      "metadata": {
        "id": "eYCo-rFWmNm3",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          },
          "output_extras": [
            {
              "item_id": 6
            }
          ],
          "base_uri": "https://localhost:8080/",
          "height": 204
        },
        "outputId": "23fb8264-505d-4349-b686-97823773c5d1",
        "executionInfo": {
          "status": "ok",
          "timestamp": 1520718725267,
          "user_tz": 300,
          "elapsed": 37994,
          "user": {
            "displayName": "Adhish Thite",
            "photoUrl": "//lh5.googleusercontent.com/-Mrh4dvnXd5c/AAAAAAAAAAI/AAAAAAAAX2M/O-s14hFhqXA/s50-c-k-no/photo.jpg",
            "userId": "105913067845686448749"
          }
        }
      },
      "cell_type": "code",
      "source": [
        "## IMPORTS\n",
        "\n",
        "# These are all the modules we'll be using later. Make sure you can import them\n",
        "# before proceeding further.\n",
        "from __future__ import print_function\n",
        "import tensorflow as tf\n",
        "from six.moves import cPickle as pickle\n",
        "from six.moves import range\n",
        "import numpy as np\n",
        "import os\n",
        "import sys\n",
        "import tarfile\n",
        "from IPython.display import display, Image\n",
        "from scipy import ndimage\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from six.moves.urllib.request import urlretrieve\n",
        "from six.moves import cPickle as pickle\n",
        "\n",
        "!pip install imageio\n",
        "\n",
        "import imageio\n",
        "\n",
        "# Config the matlotlib backend as plotting inline in IPython\n",
        "%matplotlib inline\n",
        "\n",
        "## DATA SETUP\n",
        "\n",
        "# FUSE Drive to access Drive Data\n",
        "!apt-get install -y -qq software-properties-common python-software-properties module-init-tools\n",
        "!add-apt-repository -y ppa:alessandro-strada/ppa 2>&1 > /dev/null\n",
        "!apt-get update -qq 2>&1 > /dev/null\n",
        "!apt-get -y install -qq google-drive-ocamlfuse fuse\n",
        "\n",
        "# Generate auth tokens for Colab\n",
        "from google.colab import auth\n",
        "auth.authenticate_user()\n",
        "\n",
        "# Generate creds for the Drive FUSE library.\n",
        "from oauth2client.client import GoogleCredentials\n",
        "creds = GoogleCredentials.get_application_default()\n",
        "import getpass\n",
        "!google-drive-ocamlfuse -headless -id={creds.client_id} -secret={creds.client_secret} < /dev/null 2>&1 | grep URL\n",
        "vcode = getpass.getpass()\n",
        "!echo {vcode} | google-drive-ocamlfuse -headless -id={creds.client_id} -secret={creds.client_secret}"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: imageio in /usr/local/lib/python3.6/dist-packages\r\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from imageio)\r\n",
            "Requirement already satisfied: pillow in /usr/local/lib/python3.6/dist-packages (from imageio)\r\n",
            "Requirement already satisfied: olefile in /usr/local/lib/python3.6/dist-packages (from pillow->imageio)\n",
            "gpg: keybox '/tmp/tmp718zm2mj/pubring.gpg' created\n",
            "gpg: /tmp/tmp718zm2mj/trustdb.gpg: trustdb created\n",
            "gpg: key AD5F235DF639B041: public key \"Launchpad PPA for Alessandro Strada\" imported\n",
            "gpg: Total number processed: 1\n",
            "gpg:               imported: 1\n",
            "Warning: apt-key output should not be parsed (stdout is not a terminal)\n",
            "··········\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "Nq7lNg3JNmlW",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          },
          "output_extras": [
            {
              "item_id": 2
            }
          ],
          "base_uri": "https://localhost:8080/",
          "height": 119
        },
        "outputId": "35a20bfc-d2f5-49a4-8550-e3753c7a37a9",
        "executionInfo": {
          "status": "ok",
          "timestamp": 1520718734128,
          "user_tz": 300,
          "elapsed": 8833,
          "user": {
            "displayName": "Adhish Thite",
            "photoUrl": "//lh5.googleusercontent.com/-Mrh4dvnXd5c/AAAAAAAAAAI/AAAAAAAAX2M/O-s14hFhqXA/s50-c-k-no/photo.jpg",
            "userId": "105913067845686448749"
          }
        }
      },
      "cell_type": "code",
      "source": [
        "# Create a directory and mount Google Drive using that directory.\n",
        "!mkdir -p drive\n",
        "!google-drive-ocamlfuse drive\n",
        "\n",
        "pickle_file = 'drive/Data/notMNIST.pickle'\n",
        "\n",
        "with open(pickle_file, 'rb') as f:\n",
        "  save = pickle.load(f)\n",
        "  train_dataset = save['train_dataset']\n",
        "  train_labels = save['train_labels']\n",
        "  valid_dataset = save['valid_dataset']\n",
        "  valid_labels = save['valid_labels']\n",
        "  test_dataset = save['test_dataset']\n",
        "  test_labels = save['test_labels']\n",
        "  del save  # hint to help gc free up memory\n",
        "\n",
        "image_size = 28\n",
        "num_labels = 10\n",
        "\n",
        "def reformat(dataset, labels):\n",
        "  dataset = dataset.reshape((-1, image_size * image_size)).astype(np.float32)\n",
        "  # Map 2 to [0.0, 1.0, 0.0 ...], 3 to [0.0, 0.0, 1.0 ...]\n",
        "  labels = (np.arange(num_labels) == labels[:,None]).astype(np.float32)\n",
        "  return dataset, labels\n",
        "\n",
        "train_dataset, train_labels = reformat(train_dataset, train_labels)\n",
        "valid_dataset, valid_labels = reformat(valid_dataset, valid_labels)\n",
        "test_dataset, test_labels = reformat(test_dataset, test_labels)\n",
        "\n",
        "print('\\nTraining set', train_dataset.shape, train_labels.shape)\n",
        "print('Validation set', valid_dataset.shape, valid_labels.shape)\n",
        "print('Test set', test_dataset.shape, test_labels.shape)"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "fuse: mountpoint is not empty\r\n",
            "fuse: if you are sure this is safe, use the 'nonempty' mount option\r\n",
            "\n",
            "Training set (200000, 784) (200000, 10)\n",
            "Validation set (10000, 784) (10000, 10)\n",
            "Test set (10000, 784) (10000, 10)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "ria0p3Pp6jAD",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Accuracy\n",
        "\n",
        "Defining an accuracy function"
      ]
    },
    {
      "metadata": {
        "id": "d9n4Nesh47rp",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          }
        }
      },
      "cell_type": "code",
      "source": [
        "def accuracy(predictions, labels):\n",
        "  return (100.0 * np.sum(np.argmax(predictions, 1) == np.argmax(labels, 1))\n",
        "          / predictions.shape[0])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "ABDSp2YQ6qFh",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "---"
      ]
    },
    {
      "metadata": {
        "id": "qhhRALqO6xRj",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Step 2 - L2 Regularization"
      ]
    },
    {
      "metadata": {
        "id": "iS5LuLN56reJ",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Problem 1\n",
        "\n",
        "Introduce and tune L2 regularization for both logistic and neural network models. Remember that L2 amounts to adding a penalty on the norm of the weights to the loss. In TensorFlow, you can compute the L2 loss for a tensor t using nn.l2_loss(t). The right amount of regularization should improve your validation / test accuracy."
      ]
    },
    {
      "metadata": {
        "id": "gX44gRDTMDW1",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Multinomial Logistic Regression"
      ]
    },
    {
      "metadata": {
        "id": "JpbXnubO6oHo",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          }
        }
      },
      "cell_type": "code",
      "source": [
        "# With gradient descent training, even this much data is prohibitive.\n",
        "# Subset the training data for faster turnaround.\n",
        "train_subset = 10000\n",
        "beta = 0.01\n",
        "\n",
        "graph = tf.Graph()\n",
        "with graph.as_default():\n",
        "\n",
        "  # Input data.\n",
        "  # Load the training, validation and test data into constants that are\n",
        "  # attached to the graph.\n",
        "  tf_train_dataset = tf.constant(train_dataset[:train_subset, :])\n",
        "  tf_train_labels = tf.constant(train_labels[:train_subset])\n",
        "  tf_valid_dataset = tf.constant(valid_dataset)\n",
        "  tf_test_dataset = tf.constant(test_dataset)\n",
        "  \n",
        "  # Variables.\n",
        "  # These are the parameters that we are going to be training. The weight\n",
        "  # matrix will be initialized using random values following a (truncated)\n",
        "  # normal distribution. The biases get initialized to zero.\n",
        "  weights = tf.Variable(\n",
        "    tf.truncated_normal([image_size * image_size, num_labels]))\n",
        "  biases = tf.Variable(tf.zeros([num_labels]))\n",
        "  \n",
        "  # Training computation.\n",
        "  # We multiply the inputs with the weight matrix, and add biases. We compute\n",
        "  # the softmax and cross-entropy (it's one operation in TensorFlow, because\n",
        "  # it's very common, and it can be optimized). We take the average of this\n",
        "  # cross-entropy across all training examples: that's our loss.\n",
        "  logits = tf.matmul(tf_train_dataset, weights) + biases\n",
        "  loss = tf.reduce_mean(\n",
        "    tf.nn.softmax_cross_entropy_with_logits_v2(labels=tf_train_labels, logits=logits))\n",
        "  \n",
        "  ## Loss Function using L2 regularization\n",
        "  reg = tf.nn.l2_loss(weights)\n",
        "  loss = tf.reduce_mean(loss + beta * reg)\n",
        "  \n",
        "  # Optimizer.\n",
        "  # We are going to find the minimum of this loss using gradient descent.\n",
        "  optimizer = tf.train.GradientDescentOptimizer(0.5).minimize(loss)\n",
        "  \n",
        "  # Predictions for the training, validation, and test data.\n",
        "  # These are not part of training, but merely here so that we can report\n",
        "  # accuracy figures as we train.\n",
        "  train_prediction = tf.nn.softmax(logits)\n",
        "  valid_prediction = tf.nn.softmax(tf.matmul(tf_valid_dataset, weights) + biases)\n",
        "  test_prediction = tf.nn.softmax(tf.matmul(tf_test_dataset, weights) + biases)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "4nzNpOktNGP0",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          },
          "output_extras": [
            {
              "item_id": 13
            }
          ],
          "base_uri": "https://localhost:8080/",
          "height": 921
        },
        "outputId": "b7aa5865-e304-4f30-aba2-2dd9b6272328",
        "executionInfo": {
          "status": "ok",
          "timestamp": 1520718748419,
          "user_tz": 300,
          "elapsed": 13460,
          "user": {
            "displayName": "Adhish Thite",
            "photoUrl": "//lh5.googleusercontent.com/-Mrh4dvnXd5c/AAAAAAAAAAI/AAAAAAAAX2M/O-s14hFhqXA/s50-c-k-no/photo.jpg",
            "userId": "105913067845686448749"
          }
        }
      },
      "cell_type": "code",
      "source": [
        "num_steps = 5001\n",
        "\n",
        "def accuracy(predictions, labels):\n",
        "    return (100.0 * np.sum(np.argmax(predictions, 1) == np.argmax(labels, 1))\n",
        "            / predictions.shape[0])\n",
        "\n",
        "with tf.Session(graph=graph) as session:\n",
        "    # This is a one-time operation which ensures the parameters get initialized as\n",
        "    # we described in the graph: random weights for the matrix, zeros for the\n",
        "    # biases. \n",
        "    tf.initialize_all_variables().run()\n",
        "    \n",
        "    print('\\nInitialized\\n')\n",
        "    \n",
        "    for step in range(num_steps):\n",
        "    # Run the computations. We tell .run() that we want to run the optimizer,\n",
        "    # and get the loss value and the training predictions returned as numpy\n",
        "    # arrays.\n",
        "        _, l, predictions = session.run([optimizer, loss, train_prediction])\n",
        "        if (step % 500 == 0):\n",
        "            print('\\nLoss at step {}: {}'.format(step, l))\n",
        "            print('Training accuracy: {:.1f}'.format(accuracy(predictions, \n",
        "                                                         train_labels[:train_subset, :])))\n",
        "            # Calling .eval() on valid_prediction is basically like calling run(), but\n",
        "            # just to get that one numpy array. Note that it recomputes all its graph\n",
        "            # dependencies.\n",
        "            \n",
        "            # You don't have to do .eval above because we already ran the session for the\n",
        "            # train_prediction\n",
        "            print('Validation accuracy: {:.1f}'.format(accuracy(valid_prediction.eval(), \n",
        "                                                           valid_labels)))\n",
        "    print('\\nTest accuracy: {:.1f}'.format(accuracy(test_prediction.eval(), test_labels)))"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/util/tf_should_use.py:118: initialize_all_variables (from tensorflow.python.ops.variables) is deprecated and will be removed after 2017-03-02.\n",
            "Instructions for updating:\n",
            "Use `tf.global_variables_initializer` instead.\n",
            "\n",
            "Initialized\n",
            "\n",
            "\n",
            "Loss at step 0: 45.35413360595703\n",
            "Training accuracy: 14.3\n",
            "Validation accuracy: 18.1\n",
            "\n",
            "Loss at step 500: 0.8454488515853882\n",
            "Training accuracy: 84.1\n",
            "Validation accuracy: 82.1\n",
            "\n",
            "Loss at step 1000: 0.6894906759262085\n",
            "Training accuracy: 84.5\n",
            "Validation accuracy: 82.3\n",
            "\n",
            "Loss at step 1500: 0.6886019706726074\n",
            "Training accuracy: 84.4\n",
            "Validation accuracy: 82.3\n",
            "\n",
            "Loss at step 2000: 0.688593327999115\n",
            "Training accuracy: 84.5\n",
            "Validation accuracy: 82.3\n",
            "\n",
            "Loss at step 2500: 0.6885926723480225\n",
            "Training accuracy: 84.4\n",
            "Validation accuracy: 82.3\n",
            "\n",
            "Loss at step 3000: 0.6885926127433777\n",
            "Training accuracy: 84.4\n",
            "Validation accuracy: 82.3\n",
            "\n",
            "Loss at step 3500: 0.6885926723480225\n",
            "Training accuracy: 84.4\n",
            "Validation accuracy: 82.3\n",
            "\n",
            "Loss at step 4000: 0.6885926127433777\n",
            "Training accuracy: 84.4\n",
            "Validation accuracy: 82.3\n",
            "\n",
            "Loss at step 4500: 0.6885926127433777\n",
            "Training accuracy: 84.4\n",
            "Validation accuracy: 82.3\n",
            "\n",
            "Loss at step 5000: 0.6885926127433777\n",
            "Training accuracy: 84.4\n",
            "Validation accuracy: 82.3\n",
            "\n",
            "Test accuracy: 88.6\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "m_LkYv6dPbxa",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### One Layer Neural Network"
      ]
    },
    {
      "metadata": {
        "id": "ge2G9IE5Om4T",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          }
        }
      },
      "cell_type": "code",
      "source": [
        "num_nodes= 1024\n",
        "batch_size = 128\n",
        "beta = 0.001\n",
        "\n",
        "graph = tf.Graph()\n",
        "with graph.as_default():\n",
        "\n",
        "    # Input data. For the training data, we use a placeholder that will be fed\n",
        "    # at run time with a training minibatch.\n",
        "    tf_train_dataset = tf.placeholder(tf.float32, shape=(batch_size, image_size * image_size))\n",
        "    tf_train_labels = tf.placeholder(tf.float32, shape=(batch_size, num_labels))\n",
        "    tf_valid_dataset = tf.constant(valid_dataset)\n",
        "    tf_test_dataset = tf.constant(test_dataset)\n",
        "\n",
        "    # Variables.\n",
        "    weights_1 = tf.Variable(tf.truncated_normal([image_size * image_size, num_nodes]))\n",
        "    biases_1 = tf.Variable(tf.zeros([num_nodes]))\n",
        "    weights_2 = tf.Variable(tf.truncated_normal([num_nodes, num_labels]))\n",
        "    biases_2 = tf.Variable(tf.zeros([num_labels]))\n",
        "\n",
        "    # Training computation.\n",
        "    logits_1 = tf.matmul(tf_train_dataset, weights_1) + biases_1\n",
        "    relu_layer= tf.nn.relu(logits_1)\n",
        "    logits_2 = tf.matmul(relu_layer, weights_2) + biases_2\n",
        "    \n",
        "    # Normal loss function\n",
        "    loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits_v2(logits=logits_2, labels=tf_train_labels))\n",
        "    \n",
        "    # Loss function with L2 Regularization with beta=0.01\n",
        "    regularizers = tf.nn.l2_loss(weights_1) + tf.nn.l2_loss(weights_2)\n",
        "    loss = tf.reduce_mean(loss + beta * regularizers)\n",
        "\n",
        "    # Optimizer.\n",
        "    optimizer = tf.train.GradientDescentOptimizer(0.5).minimize(loss)\n",
        "\n",
        "    # Predictions for the training\n",
        "    train_prediction = tf.nn.softmax(logits_2)\n",
        "    \n",
        "    # Predictions for validation \n",
        "    logits_1 = tf.matmul(tf_valid_dataset, weights_1) + biases_1\n",
        "    relu_layer= tf.nn.relu(logits_1)\n",
        "    logits_2 = tf.matmul(relu_layer, weights_2) + biases_2\n",
        "    \n",
        "    valid_prediction = tf.nn.softmax(logits_2)\n",
        "    \n",
        "    # Predictions for test\n",
        "    logits_1 = tf.matmul(tf_test_dataset, weights_1) + biases_1\n",
        "    relu_layer= tf.nn.relu(logits_1)\n",
        "    logits_2 = tf.matmul(relu_layer, weights_2) + biases_2\n",
        "    \n",
        "    test_prediction =  tf.nn.softmax(logits_2)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "A-3IX-oOUagO",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          },
          "output_extras": [
            {
              "item_id": 11
            }
          ],
          "base_uri": "https://localhost:8080/",
          "height": 833
        },
        "outputId": "8f469fe0-07ea-4743-b9f0-2549388370d9",
        "executionInfo": {
          "status": "ok",
          "timestamp": 1520718773797,
          "user_tz": 300,
          "elapsed": 24726,
          "user": {
            "displayName": "Adhish Thite",
            "photoUrl": "//lh5.googleusercontent.com/-Mrh4dvnXd5c/AAAAAAAAAAI/AAAAAAAAX2M/O-s14hFhqXA/s50-c-k-no/photo.jpg",
            "userId": "105913067845686448749"
          }
        }
      },
      "cell_type": "code",
      "source": [
        "num_steps = 10001\n",
        "\n",
        "with tf.Session(graph=graph) as session:\n",
        "    tf.initialize_all_variables().run()\n",
        "    print(\"\\nInitialized\")\n",
        "    for step in range(num_steps):\n",
        "        # Pick an offset within the training data, which has been randomized.\n",
        "        # Note: we could use better randomization across epochs.\n",
        "        offset = (step * batch_size) % (train_labels.shape[0] - batch_size)\n",
        "        \n",
        "        # Generate a minibatch.\n",
        "        batch_data = train_dataset[offset:(offset + batch_size), :]\n",
        "        batch_labels = train_labels[offset:(offset + batch_size), :]\n",
        "        \n",
        "        # Prepare a dictionary telling the session where to feed the minibatch.\n",
        "        # The key of the dictionary is the placeholder node of the graph to be fed,\n",
        "        # and the value is the numpy array to feed to it.\n",
        "        feed_dict = {tf_train_dataset : batch_data, tf_train_labels : batch_labels}\n",
        "        _, l, predictions = session.run([optimizer, loss, train_prediction], feed_dict=feed_dict)\n",
        "        \n",
        "        if (step % 1000 == 0):\n",
        "            print(\"\\nMinibatch loss at step {}: {}\".format(step, l))\n",
        "            print(\"Minibatch accuracy: {:.1f}\".format(accuracy(predictions, batch_labels)))\n",
        "            print(\"Validation accuracy: {:.1f}\".format(accuracy(valid_prediction.eval(), valid_labels)))\n",
        "    print(\"\\nTest accuracy: {:.1f}\".format(accuracy(test_prediction.eval(), test_labels)))"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Initialized\n",
            "\n",
            "Minibatch loss at step 0: 569.9794921875\n",
            "Minibatch accuracy: 12.5\n",
            "Validation accuracy: 27.5\n",
            "\n",
            "Minibatch loss at step 1000: 114.4655990600586\n",
            "Minibatch accuracy: 83.6\n",
            "Validation accuracy: 81.1\n",
            "\n",
            "Minibatch loss at step 2000: 41.4669303894043\n",
            "Minibatch accuracy: 85.9\n",
            "Validation accuracy: 85.2\n",
            "\n",
            "Minibatch loss at step 3000: 15.450621604919434\n",
            "Minibatch accuracy: 88.3\n",
            "Validation accuracy: 87.3\n",
            "\n",
            "Minibatch loss at step 4000: 6.124664306640625\n",
            "Minibatch accuracy: 85.9\n",
            "Validation accuracy: 88.4\n",
            "\n",
            "Minibatch loss at step 5000: 2.5495200157165527\n",
            "Minibatch accuracy: 86.7\n",
            "Validation accuracy: 88.3\n",
            "\n",
            "Minibatch loss at step 6000: 1.300624132156372\n",
            "Minibatch accuracy: 85.9\n",
            "Validation accuracy: 88.5\n",
            "\n",
            "Minibatch loss at step 7000: 0.8731435537338257\n",
            "Minibatch accuracy: 86.7\n",
            "Validation accuracy: 88.4\n",
            "\n",
            "Minibatch loss at step 8000: 0.6539442539215088\n",
            "Minibatch accuracy: 89.1\n",
            "Validation accuracy: 88.6\n",
            "\n",
            "Minibatch loss at step 9000: 0.5593432784080505\n",
            "Minibatch accuracy: 88.3\n",
            "Validation accuracy: 89.1\n",
            "\n",
            "Minibatch loss at step 10000: 0.6061401963233948\n",
            "Minibatch accuracy: 87.5\n",
            "Validation accuracy: 88.5\n",
            "\n",
            "Test accuracy: 93.8\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "nS7JTuoiWX2e",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Problem 2\n",
        "\n",
        "Let's demonstrate an extreme case of overfitting. Restrict your training data to just a few batches. What happens?"
      ]
    },
    {
      "metadata": {
        "id": "QnXn6PPrV0K8",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          },
          "output_extras": [
            {
              "item_id": 11
            }
          ],
          "base_uri": "https://localhost:8080/",
          "height": 833
        },
        "outputId": "80ba9581-b79b-4588-b294-19044cb34730",
        "executionInfo": {
          "status": "ok",
          "timestamp": 1520718786858,
          "user_tz": 300,
          "elapsed": 13029,
          "user": {
            "displayName": "Adhish Thite",
            "photoUrl": "//lh5.googleusercontent.com/-Mrh4dvnXd5c/AAAAAAAAAAI/AAAAAAAAX2M/O-s14hFhqXA/s50-c-k-no/photo.jpg",
            "userId": "105913067845686448749"
          }
        }
      },
      "cell_type": "code",
      "source": [
        "num_steps = 5001\n",
        "\n",
        "train_dataset_2 = train_dataset[:500, :]\n",
        "train_labels_2 = train_labels[:500]\n",
        "\n",
        "with tf.Session(graph=graph) as session:\n",
        "    tf.initialize_all_variables().run()\n",
        "    print(\"\\nInitialized\")\n",
        "    for step in range(num_steps):\n",
        "        # Pick an offset within the training data, which has been randomized.\n",
        "        # Note: we could use better randomization across epochs.\n",
        "        offset = (step * batch_size) % (train_labels_2.shape[0] - batch_size)\n",
        "        \n",
        "        # Generate a minibatch.\n",
        "        batch_data = train_dataset_2[offset:(offset + batch_size), :]\n",
        "        batch_labels = train_labels_2[offset:(offset + batch_size), :]\n",
        "        \n",
        "        # Prepare a dictionary telling the session where to feed the minibatch.\n",
        "        # The key of the dictionary is the placeholder node of the graph to be fed,\n",
        "        # and the value is the numpy array to feed to it.\n",
        "        \n",
        "        feed_dict = {tf_train_dataset : batch_data, tf_train_labels : batch_labels}\n",
        "        _, l, predictions = session.run([optimizer, loss, train_prediction], feed_dict=feed_dict)\n",
        "        if (step % 500 == 0):\n",
        "            print(\"\\nMinibatch loss at step {}: {}\".format(step, l))\n",
        "            print(\"Minibatch accuracy: {:.1f}\".format(accuracy(predictions, batch_labels)))\n",
        "            print(\"Validation accuracy: {:.1f}\".format(accuracy(valid_prediction.eval(), valid_labels)))\n",
        "    print(\"\\nTest accuracy: {:.1f}\".format(accuracy(test_prediction.eval(), test_labels)))"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Initialized\n",
            "\n",
            "Minibatch loss at step 0: 743.8604736328125\n",
            "Minibatch accuracy: 9.4\n",
            "Validation accuracy: 31.9\n",
            "\n",
            "Minibatch loss at step 500: 190.55657958984375\n",
            "Minibatch accuracy: 100.0\n",
            "Validation accuracy: 74.3\n",
            "\n",
            "Minibatch loss at step 1000: 115.56396484375\n",
            "Minibatch accuracy: 100.0\n",
            "Validation accuracy: 74.3\n",
            "\n",
            "Minibatch loss at step 1500: 70.0843276977539\n",
            "Minibatch accuracy: 100.0\n",
            "Validation accuracy: 74.3\n",
            "\n",
            "Minibatch loss at step 2000: 42.503047943115234\n",
            "Minibatch accuracy: 100.0\n",
            "Validation accuracy: 74.4\n",
            "\n",
            "Minibatch loss at step 2500: 25.776491165161133\n",
            "Minibatch accuracy: 100.0\n",
            "Validation accuracy: 74.5\n",
            "\n",
            "Minibatch loss at step 3000: 15.633467674255371\n",
            "Minibatch accuracy: 100.0\n",
            "Validation accuracy: 75.0\n",
            "\n",
            "Minibatch loss at step 3500: 9.484251022338867\n",
            "Minibatch accuracy: 100.0\n",
            "Validation accuracy: 75.7\n",
            "\n",
            "Minibatch loss at step 4000: 5.758632183074951\n",
            "Minibatch accuracy: 100.0\n",
            "Validation accuracy: 76.7\n",
            "\n",
            "Minibatch loss at step 4500: 3.503783702850342\n",
            "Minibatch accuracy: 100.0\n",
            "Validation accuracy: 77.7\n",
            "\n",
            "Minibatch loss at step 5000: 2.140507698059082\n",
            "Minibatch accuracy: 100.0\n",
            "Validation accuracy: 78.3\n",
            "\n",
            "Test accuracy: 85.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "sJFKFXbEfWNR",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "High training accuracy, and low validation accuracy - Overfitting.\n",
        "\n",
        "---"
      ]
    },
    {
      "metadata": {
        "id": "WFsZhqj4flHh",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Problem 3\n",
        "\n",
        "Introduce Dropout on the hidden layer of the neural network. Remember: Dropout should only be introduced during training, not evaluation, otherwise your evaluation results would be stochastic as well. TensorFlow provides nn.dropout() for that, but you have to make sure it's only inserted during training.\n",
        "\n",
        "\n",
        "What happens to our extreme overfitting case?"
      ]
    },
    {
      "metadata": {
        "id": "ATzZGpIjc-1D",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          }
        }
      },
      "cell_type": "code",
      "source": [
        "num_nodes= 1024\n",
        "batch_size = 128\n",
        "beta = 0.01\n",
        "\n",
        "graph = tf.Graph()\n",
        "with graph.as_default():\n",
        "\n",
        "    # Input data. For the training data, we use a placeholder that will be fed\n",
        "    # at run time with a training minibatch.\n",
        "    tf_train_dataset = tf.placeholder(tf.float32, shape=(batch_size, image_size * image_size))\n",
        "    tf_train_labels = tf.placeholder(tf.float32, shape=(batch_size, num_labels))\n",
        "    tf_valid_dataset = tf.constant(valid_dataset)\n",
        "    tf_test_dataset = tf.constant(test_dataset)\n",
        "\n",
        "    # Variables.\n",
        "    weights_1 = tf.Variable(tf.truncated_normal([image_size * image_size, num_nodes]))\n",
        "    biases_1 = tf.Variable(tf.zeros([num_nodes]))\n",
        "    weights_2 = tf.Variable(tf.truncated_normal([num_nodes, num_labels]))\n",
        "    biases_2 = tf.Variable(tf.zeros([num_labels]))\n",
        "    \n",
        "    # Training computation.\n",
        "    logits_1 = tf.matmul(tf_train_dataset, weights_1) + biases_1\n",
        "    relu_layer= tf.nn.relu(logits_1)\n",
        "    \n",
        "    # Dropout on hidden layer: RELU layer\n",
        "    keep_prob = tf.placeholder(\"float\")\n",
        "    relu_layer_dropout = tf.nn.dropout(relu_layer, keep_prob)\n",
        "    \n",
        "    logits_2 = tf.matmul(relu_layer_dropout, weights_2) + biases_2\n",
        "    \n",
        "    # Normal loss function\n",
        "    loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits_v2(logits=logits_2, labels=tf_train_labels))\n",
        "    \n",
        "    # Loss function with L2 Regularization with beta=0.01\n",
        "    regularizers = tf.nn.l2_loss(weights_1) + tf.nn.l2_loss(weights_2)\n",
        "    loss = tf.reduce_mean(loss + beta * regularizers)\n",
        "\n",
        "    # Optimizer.\n",
        "    optimizer = tf.train.GradientDescentOptimizer(0.5).minimize(loss)\n",
        "\n",
        "    # Predictions for the training\n",
        "    train_prediction = tf.nn.softmax(logits_2)\n",
        "    \n",
        "    # Predictions for validation \n",
        "    logits_1 = tf.matmul(tf_valid_dataset, weights_1) + biases_1\n",
        "    relu_layer= tf.nn.relu(logits_1)\n",
        "    logits_2 = tf.matmul(relu_layer, weights_2) + biases_2\n",
        "    \n",
        "    valid_prediction = tf.nn.softmax(logits_2)\n",
        "    \n",
        "    # Predictions for test\n",
        "    logits_1 = tf.matmul(tf_test_dataset, weights_1) + biases_1\n",
        "    relu_layer= tf.nn.relu(logits_1)\n",
        "    logits_2 = tf.matmul(relu_layer, weights_2) + biases_2\n",
        "    \n",
        "    test_prediction =  tf.nn.softmax(logits_2)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "iSv5QhDOiPL3",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          },
          "output_extras": [
            {
              "item_id": 11
            }
          ],
          "base_uri": "https://localhost:8080/",
          "height": 833
        },
        "outputId": "34572c37-3940-4a73-db37-936318e62ff5",
        "executionInfo": {
          "status": "ok",
          "timestamp": 1520718802077,
          "user_tz": 300,
          "elapsed": 14534,
          "user": {
            "displayName": "Adhish Thite",
            "photoUrl": "//lh5.googleusercontent.com/-Mrh4dvnXd5c/AAAAAAAAAAI/AAAAAAAAX2M/O-s14hFhqXA/s50-c-k-no/photo.jpg",
            "userId": "105913067845686448749"
          }
        }
      },
      "cell_type": "code",
      "source": [
        "num_steps = 5001\n",
        "\n",
        "with tf.Session(graph=graph) as session:\n",
        "    tf.initialize_all_variables().run()\n",
        "    print(\"\\nInitialized\")\n",
        "    \n",
        "    for step in range(num_steps):\n",
        "        # Pick an offset within the training data, which has been randomized.\n",
        "        # Note: we could use better randomization across epochs.\n",
        "        offset = (step * batch_size) % (train_labels.shape[0] - batch_size)\n",
        "        \n",
        "        # Generate a minibatch.\n",
        "        batch_data = train_dataset[offset:(offset + batch_size), :]\n",
        "        batch_labels = train_labels[offset:(offset + batch_size), :]\n",
        "        \n",
        "        # Prepare a dictionary telling the session where to feed the minibatch.\n",
        "        # The key of the dictionary is the placeholder node of the graph to be fed,\n",
        "        # and the value is the numpy array to feed to it.\n",
        "        feed_dict = {tf_train_dataset : batch_data, tf_train_labels : batch_labels, keep_prob : 0.5}\n",
        "        _, l, predictions = session.run([optimizer, loss, train_prediction], feed_dict=feed_dict)\n",
        "        \n",
        "        if (step % 500 == 0):\n",
        "            print(\"\\nMinibatch loss at step {}: {}\".format(step, l))\n",
        "            print(\"Minibatch accuracy: {:.1f}\".format(accuracy(predictions, batch_labels)))\n",
        "            print(\"Validation accuracy: {:.1f}\".format(accuracy(valid_prediction.eval(), valid_labels)))\n",
        "    print(\"\\nTest accuracy: {:.1f}\".format(accuracy(test_prediction.eval(), test_labels)))"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Initialized\n",
            "\n",
            "Minibatch loss at step 0: 3628.618408203125\n",
            "Minibatch accuracy: 7.8\n",
            "Validation accuracy: 30.8\n",
            "\n",
            "Minibatch loss at step 500: 21.480159759521484\n",
            "Minibatch accuracy: 81.2\n",
            "Validation accuracy: 83.9\n",
            "\n",
            "Minibatch loss at step 1000: 1.063875675201416\n",
            "Minibatch accuracy: 81.2\n",
            "Validation accuracy: 84.1\n",
            "\n",
            "Minibatch loss at step 1500: 0.9375864267349243\n",
            "Minibatch accuracy: 77.3\n",
            "Validation accuracy: 83.4\n",
            "\n",
            "Minibatch loss at step 2000: 0.796303391456604\n",
            "Minibatch accuracy: 85.2\n",
            "Validation accuracy: 83.5\n",
            "\n",
            "Minibatch loss at step 2500: 0.8694121241569519\n",
            "Minibatch accuracy: 80.5\n",
            "Validation accuracy: 83.1\n",
            "\n",
            "Minibatch loss at step 3000: 0.7428845167160034\n",
            "Minibatch accuracy: 85.9\n",
            "Validation accuracy: 83.7\n",
            "\n",
            "Minibatch loss at step 3500: 0.7343540191650391\n",
            "Minibatch accuracy: 83.6\n",
            "Validation accuracy: 83.0\n",
            "\n",
            "Minibatch loss at step 4000: 0.9939125180244446\n",
            "Minibatch accuracy: 80.5\n",
            "Validation accuracy: 82.6\n",
            "\n",
            "Minibatch loss at step 4500: 0.7109951376914978\n",
            "Minibatch accuracy: 84.4\n",
            "Validation accuracy: 83.7\n",
            "\n",
            "Minibatch loss at step 5000: 0.7708562016487122\n",
            "Minibatch accuracy: 82.0\n",
            "Validation accuracy: 83.7\n",
            "\n",
            "Test accuracy: 89.5\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "4Nud-KuLkJ_P",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "#### Extreme Overfitting"
      ]
    },
    {
      "metadata": {
        "id": "nUT39Y_lijbe",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          },
          "output_extras": [
            {
              "item_id": 11
            }
          ],
          "base_uri": "https://localhost:8080/",
          "height": 833
        },
        "outputId": "a58b8af1-76ed-4d7d-dcec-13cf85d99a2b",
        "executionInfo": {
          "status": "ok",
          "timestamp": 1520718816601,
          "user_tz": 300,
          "elapsed": 14497,
          "user": {
            "displayName": "Adhish Thite",
            "photoUrl": "//lh5.googleusercontent.com/-Mrh4dvnXd5c/AAAAAAAAAAI/AAAAAAAAX2M/O-s14hFhqXA/s50-c-k-no/photo.jpg",
            "userId": "105913067845686448749"
          }
        }
      },
      "cell_type": "code",
      "source": [
        "num_steps = 5001\n",
        "beta = 0.01\n",
        "\n",
        "train_dataset_2 = train_dataset[:500, :]\n",
        "train_labels_2 = train_labels[:500]\n",
        "\n",
        "with tf.Session(graph=graph) as session:\n",
        "    \n",
        "    tf.initialize_all_variables().run()\n",
        "    print(\"\\nInitialized\")\n",
        "    \n",
        "    for step in range(num_steps):\n",
        "        # Pick an offset within the training data, which has been randomized.\n",
        "        # Note: we could use better randomization across epochs.\n",
        "        offset = (step * batch_size) % (train_labels_2.shape[0] - batch_size)\n",
        "        \n",
        "        # Generate a minibatch.\n",
        "        batch_data = train_dataset_2[offset:(offset + batch_size), :]\n",
        "        batch_labels = train_labels_2[offset:(offset + batch_size), :]\n",
        "        \n",
        "        # Prepare a dictionary telling the session where to feed the minibatch.\n",
        "        # The key of the dictionary is the placeholder node of the graph to be fed,\n",
        "        # and the value is the numpy array to feed to it.\n",
        "        feed_dict = {tf_train_dataset : batch_data, tf_train_labels : batch_labels,  keep_prob : 0.5}\n",
        "        _, l, predictions = session.run([optimizer, loss, train_prediction], feed_dict=feed_dict)\n",
        "        \n",
        "        if (step % 500 == 0):\n",
        "            print(\"\\nMinibatch loss at step {}: {}\".format(step, l))\n",
        "            print(\"Minibatch accuracy: {:.1f}\".format(accuracy(predictions, batch_labels)))\n",
        "            print(\"Validation accuracy: {:.1f}\".format(accuracy(valid_prediction.eval(), valid_labels)))\n",
        "    print(\"\\nTest accuracy: {:.1f}\".format(accuracy(test_prediction.eval(), test_labels)))"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Initialized\n",
            "\n",
            "Minibatch loss at step 0: 3669.89697265625\n",
            "Minibatch accuracy: 8.6\n",
            "Validation accuracy: 30.8\n",
            "\n",
            "Minibatch loss at step 500: 21.113550186157227\n",
            "Minibatch accuracy: 100.0\n",
            "Validation accuracy: 78.4\n",
            "\n",
            "Minibatch loss at step 1000: 0.49936801195144653\n",
            "Minibatch accuracy: 100.0\n",
            "Validation accuracy: 78.2\n",
            "\n",
            "Minibatch loss at step 1500: 0.31613820791244507\n",
            "Minibatch accuracy: 100.0\n",
            "Validation accuracy: 78.6\n",
            "\n",
            "Minibatch loss at step 2000: 0.3108214735984802\n",
            "Minibatch accuracy: 99.2\n",
            "Validation accuracy: 78.0\n",
            "\n",
            "Minibatch loss at step 2500: 0.29215267300605774\n",
            "Minibatch accuracy: 100.0\n",
            "Validation accuracy: 78.1\n",
            "\n",
            "Minibatch loss at step 3000: 0.28601858019828796\n",
            "Minibatch accuracy: 100.0\n",
            "Validation accuracy: 78.6\n",
            "\n",
            "Minibatch loss at step 3500: 0.29813283681869507\n",
            "Minibatch accuracy: 100.0\n",
            "Validation accuracy: 78.4\n",
            "\n",
            "Minibatch loss at step 4000: 0.2962633967399597\n",
            "Minibatch accuracy: 100.0\n",
            "Validation accuracy: 78.3\n",
            "\n",
            "Minibatch loss at step 4500: 0.29470157623291016\n",
            "Minibatch accuracy: 100.0\n",
            "Validation accuracy: 78.2\n",
            "\n",
            "Minibatch loss at step 5000: 0.29099443554878235\n",
            "Minibatch accuracy: 100.0\n",
            "Validation accuracy: 78.5\n",
            "\n",
            "Test accuracy: 85.3\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "-A0uhrCOlJ0y",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Problem 4\n",
        "\n",
        "Try to get the best performance you can using a multi-layer model! The best reported test accuracy using a deep network is 97.1%.\n",
        "\n",
        "One avenue you can explore is to add multiple layers.\n",
        "\n",
        "Another one is to use learning rate decay:\n",
        "\n",
        "> `global_step = tf.Variable(0)  # count the number of steps taken.`\n",
        "\n",
        "> `learning_rate = tf.train.exponential_decay(0.5, global_step, ...)`\n",
        "\n",
        "> `optimizer = tf.train.GradientDescentOptimizer(learning_rate).minimize(loss, global_step=global_step)`"
      ]
    },
    {
      "metadata": {
        "id": "gCbed1_C4yU3",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "#### Model -\n",
        "\n",
        "\n",
        "*   3 Hidden Layers (RELU)\n",
        "*   L2 Regularization\n",
        "*   Learning Rate Decay (Exponential)\n",
        "*   Dropout\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "metadata": {
        "id": "i-uC7Kibkqqc",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          }
        }
      },
      "cell_type": "code",
      "source": [
        "import math as math"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "pn7FF1QFmaZn",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          }
        }
      },
      "cell_type": "code",
      "source": [
        "batch_size = 128\n",
        "beta = 0.001\n",
        "\n",
        "hidden_nodes_1 = 1024\n",
        "hidden_nodes_2 = int(1024 * 0.5)\n",
        "hidden_nodes_3 = int(1024 * np.power(0.5, 2))\n",
        "\n",
        "graph = tf.Graph()\n",
        "with graph.as_default():\n",
        "  \n",
        "  '''INPUT DATA'''\n",
        "  # For the training data, we use a placeholder that will be fed\n",
        "  # at run time with a training minibatch.\n",
        "  tf_train_dataset = tf.placeholder(tf.float32, shape=(batch_size, image_size * image_size))\n",
        "  tf_train_labels = tf.placeholder(tf.float32, shape=(batch_size, num_labels))\n",
        "  tf_valid_dataset = tf.constant(valid_dataset)\n",
        "  tf_test_dataset = tf.constant(test_dataset)\n",
        "  \n",
        "  '''VARIABLES'''\n",
        "  \n",
        "  # Hidden RELU 1\n",
        "  weights_1 = tf.Variable(tf.truncated_normal([image_size * image_size, hidden_nodes_1], stddev= math.sqrt(2.0/(image_size * image_size))))\n",
        "  biases_1 = tf.Variable(tf.zeros([hidden_nodes_1]))\n",
        "  \n",
        "  # Hidden RELU 2\n",
        "  weights_2 = tf.Variable(tf.truncated_normal([hidden_nodes_1, hidden_nodes_2], stddev= math.sqrt(2.0/(hidden_nodes_1))))\n",
        "  biases_2 = tf.Variable(tf.zeros([hidden_nodes_2]))\n",
        "  \n",
        "  # Hidden RELU 1\n",
        "  weights_3 = tf.Variable(tf.truncated_normal([hidden_nodes_2, hidden_nodes_3], stddev= math.sqrt(2.0/(hidden_nodes_2))))\n",
        "  biases_3 = tf.Variable(tf.zeros([hidden_nodes_3]))\n",
        "  \n",
        "  # OUTPUT LAYER\n",
        "  weights_o = tf.Variable(tf.truncated_normal([hidden_nodes_3, num_labels], stddev=math.sqrt(2.0/(hidden_nodes_3))))\n",
        "  biases_o = tf.Variable(tf.zeros([num_labels]))\n",
        "  \n",
        "  '''TRAINING'''\n",
        "  \n",
        "  keep_prob = tf.placeholder(\"float\")\n",
        "  \n",
        "  # Hidden RELU 1\n",
        "  logits_1 = tf.matmul(tf_train_dataset, weights_1) + biases_1\n",
        "  hidden_1 = tf.nn.relu(logits_1)\n",
        "  hidden_dropout_1 = tf.nn.dropout(hidden_1, keep_prob)\n",
        "  \n",
        "  # Hidden RELU 2\n",
        "  logits_2 = tf.matmul(hidden_dropout_1, weights_2) + biases_2\n",
        "  hidden_2 = tf.nn.relu(logits_2)\n",
        "  hidden_dropout_2 = tf.nn.dropout(hidden_2, keep_prob)\n",
        "  \n",
        "  # Hidden RELU 2\n",
        "  logits_3 = tf.matmul(hidden_dropout_2, weights_3) + biases_3\n",
        "  hidden_3 = tf.nn.relu(logits_3)\n",
        "  hidden_dropout_3 = tf.nn.dropout(hidden_3, keep_prob)\n",
        "  \n",
        "  # Output Layer\n",
        "  logits_o = tf.matmul(hidden_dropout_3, weights_o) + biases_o\n",
        "  \n",
        "  '''LOSS FUNCTION'''\n",
        "  loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits_v2(logits=logits_o, labels=tf_train_labels))\n",
        "  \n",
        "  '''L2 REGULARIZATION'''\n",
        "  reg = tf.nn.l2_loss(weights_1) + tf.nn.l2_loss(weights_2) + tf.nn.l2_loss(weights_3) + tf.nn.l2_loss(weights_o)\n",
        "  loss = tf.reduce_mean(loss + beta * reg)\n",
        "  \n",
        "  '''OPTIMIZER'''\n",
        "  # Decaying Learning Rate\n",
        "  global_step = tf.Variable(0)\n",
        "  start_alpha = 0.5\n",
        "  alpha = tf.train.exponential_decay(start_alpha, global_step, 100000, 0.96, staircase=True)\n",
        "  optimizer = tf.train.GradientDescentOptimizer(alpha).minimize(loss, global_step)\n",
        "  \n",
        "  # Predictions for the training\n",
        "  train_prediction = tf.nn.softmax(logits_o)\n",
        "  \n",
        "  '''VALIDATION'''\n",
        "  valid_logits_1 = tf.matmul(tf_valid_dataset, weights_1) + biases_1\n",
        "  valid_relu_1 = tf.nn.relu(valid_logits_1)\n",
        "\n",
        "  valid_logits_2 = tf.matmul(valid_relu_1, weights_2) + biases_2\n",
        "  valid_relu_2 = tf.nn.relu(valid_logits_2)\n",
        "\n",
        "  valid_logits_3 = tf.matmul(valid_relu_2, weights_3) + biases_3\n",
        "  valid_relu_3 = tf.nn.relu(valid_logits_3)\n",
        "\n",
        "  valid_logits_o = tf.matmul(valid_relu_3, weights_o) + biases_o\n",
        "\n",
        "  valid_prediction = tf.nn.softmax(valid_logits_o)\n",
        "  \n",
        "  '''TESTING'''\n",
        "  test_logits_1 = tf.matmul(tf_test_dataset, weights_1) + biases_1\n",
        "  test_relu_1 = tf.nn.relu(test_logits_1)\n",
        "\n",
        "  test_logits_2 = tf.matmul(test_relu_1, weights_2) + biases_2\n",
        "  test_relu_2 = tf.nn.relu(test_logits_2)\n",
        "\n",
        "  test_logits_3 = tf.matmul(test_relu_2, weights_3) + biases_3\n",
        "  test_relu_3 = tf.nn.relu(test_logits_3)\n",
        "\n",
        "  test_logits_o = tf.matmul(test_relu_3, weights_o) + biases_o\n",
        "\n",
        "  test_prediction = tf.nn.softmax(test_logits_o)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "LdYPb0o73W6k",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          },
          "output_extras": [
            {
              "item_id": 11
            }
          ],
          "base_uri": "https://localhost:8080/",
          "height": 833
        },
        "outputId": "60f5b387-4d9c-47fb-cdf1-29469403a6bc",
        "executionInfo": {
          "status": "ok",
          "timestamp": 1520718938929,
          "user_tz": 300,
          "elapsed": 120916,
          "user": {
            "displayName": "Adhish Thite",
            "photoUrl": "//lh5.googleusercontent.com/-Mrh4dvnXd5c/AAAAAAAAAAI/AAAAAAAAX2M/O-s14hFhqXA/s50-c-k-no/photo.jpg",
            "userId": "105913067845686448749"
          }
        }
      },
      "cell_type": "code",
      "source": [
        "num_steps = 30001\n",
        "\n",
        "with tf.Session(graph=graph) as session:\n",
        "    \n",
        "    tf.initialize_all_variables().run()\n",
        "    print(\"\\nInitialized\")\n",
        "    \n",
        "    for step in range(num_steps):\n",
        "        # Pick an offset within the training data, which has been randomized.\n",
        "        # Note: we could use better randomization across epochs.\n",
        "        offset = (step * batch_size) % (train_labels.shape[0] - batch_size)\n",
        "        \n",
        "        # Generate a minibatch.\n",
        "        batch_data = train_dataset[offset:(offset + batch_size), :]\n",
        "        batch_labels = train_labels[offset:(offset + batch_size), :]\n",
        "        \n",
        "        # Prepare a dictionary telling the session where to feed the minibatch.\n",
        "        # The key of the dictionary is the placeholder node of the graph to be fed,\n",
        "        # and the value is the numpy array to feed to it.\n",
        "        feed_dict = {tf_train_dataset : batch_data, tf_train_labels : batch_labels, keep_prob : 0.5}\n",
        "        _, l, predictions = session.run([optimizer, loss, train_prediction], feed_dict=feed_dict)\n",
        "        \n",
        "        if (step % 3000 == 0):\n",
        "            print(\"\\nMinibatch loss at step {}: {}\".format(step, l))\n",
        "            print(\"Minibatch accuracy: {:.1f}\".format(accuracy(predictions, batch_labels)))\n",
        "            print(\"Validation accuracy: {:.1f}\".format(accuracy(valid_prediction.eval(), valid_labels)))\n",
        "    print(\"\\nTest accuracy: {:.1f}\".format(accuracy(test_prediction.eval(), test_labels)))"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Initialized\n",
            "\n",
            "Minibatch loss at step 0: 4.088729381561279\n",
            "Minibatch accuracy: 8.6\n",
            "Validation accuracy: 25.6\n",
            "\n",
            "Minibatch loss at step 3000: 0.7149103879928589\n",
            "Minibatch accuracy: 83.6\n",
            "Validation accuracy: 87.4\n",
            "\n",
            "Minibatch loss at step 6000: 0.6740361452102661\n",
            "Minibatch accuracy: 85.9\n",
            "Validation accuracy: 87.7\n",
            "\n",
            "Minibatch loss at step 9000: 0.6119821667671204\n",
            "Minibatch accuracy: 88.3\n",
            "Validation accuracy: 88.0\n",
            "\n",
            "Minibatch loss at step 12000: 0.6812489032745361\n",
            "Minibatch accuracy: 84.4\n",
            "Validation accuracy: 87.8\n",
            "\n",
            "Minibatch loss at step 15000: 0.6741139888763428\n",
            "Minibatch accuracy: 85.2\n",
            "Validation accuracy: 88.2\n",
            "\n",
            "Minibatch loss at step 18000: 0.6329600811004639\n",
            "Minibatch accuracy: 85.9\n",
            "Validation accuracy: 88.2\n",
            "\n",
            "Minibatch loss at step 21000: 0.6186057329177856\n",
            "Minibatch accuracy: 86.7\n",
            "Validation accuracy: 87.7\n",
            "\n",
            "Minibatch loss at step 24000: 0.8241613507270813\n",
            "Minibatch accuracy: 80.5\n",
            "Validation accuracy: 88.5\n",
            "\n",
            "Minibatch loss at step 27000: 0.5892763733863831\n",
            "Minibatch accuracy: 89.8\n",
            "Validation accuracy: 88.4\n",
            "\n",
            "Minibatch loss at step 30000: 0.5941524505615234\n",
            "Minibatch accuracy: 87.5\n",
            "Validation accuracy: 88.6\n",
            "\n",
            "Test accuracy: 93.8\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}